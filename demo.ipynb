{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "--- study-log.txt ---\n",
    "\n",
    "[x] 我想起來了 Siraj 的 demo.py 問題很多，\n",
    "    他是從這裡抄來的 ~\\GitHub\\tensorflow-speech-recognition\\speech2text-tflearn.py\n",
    "    [x] 1. model.fit() 所在的 while loop 亂寫，根本停不了。 --> 我的 demo2.py\n",
    "           已經處理好了。\n",
    "    [x] 2. X, y 放到了 model.fit() 的 loop 外，永遠在重複同一組 epoch\n",
    "           --> 我的 demo2.py 已經處理好了。\n",
    "    [ ] 3. Train data 跟 Test data 都是從同一鍋 data 難怪要發生 overfitting\n",
    "    [ ] 4. training_iters = 300000 早就已經 overfitting 了 --> 如何用\n",
    "           tensorboard 觀察訓練狀況。\n",
    "    [x] 5. 加上我自己的心得，跑出 memory error 的應對方法 -- 用 save-restore\n",
    "           的方法繼續 --> 我在 demo2.py 裡經由 peforth breakpoint 手動用\n",
    "           model.load() restore 當機前的 model 我好棒！\n",
    "    [x] 6. 加上 itchat 遠端監督。我當時的 itchat 遠端監督是另外跑的 WeChat rebot.\n",
    "           等於是個 peforth remote console 可以隨時抓螢幕送到遠端觀察。\n",
    "    我根據原來的 pannouse's repo 改寫成 demo2.py 加上 itchat 遠端監督。\n",
    "    所以要複習就要回憶 demo2.py ....\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"The Siraj's ~\\GitHub\\tensorflow-speech-recognition-demo\\demo.py is too bad, as well as the original pannouse's ~\\GitHub\\\\tensorflow-speech-recognition\\speech2text-tflearn.py. Let's work on my lstm-tflearn.py.ipynb instead which tops on my demo2.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified a little \n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import tflearn\n",
    "import speech_data\n",
    "import tensorflow as tf\n",
    "\n",
    "learning_rate = 0.0001\n",
    "training_iters = 300000  # steps\n",
    "batch_size = 64\n",
    "\n",
    "width = 20  # mfcc features\n",
    "height = 80  # (max) length of utterance\n",
    "classes = 10  # digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = word_batch = speech_data.mfcc_batch_generator(batch_size)\n",
    "X, Y = next(batch)\n",
    "trainX, trainY = X, Y\n",
    "testX, testY = X, Y #overfit for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network building\n",
    "net = tflearn.input_data([None, width, height])\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "net = tflearn.fully_connected(net, classes, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=learning_rate, loss='categorical_crossentropy')\n",
    "# Training\n",
    "\n",
    "### add this \"fix\" for tensorflow version errors\n",
    "col = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "for x in col:\n",
    "    tf.add_to_collection(tf.GraphKeys.VARIABLES, x ) \n",
    "\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "while 1: #training_iters\n",
    "  model.fit(trainX, trainY, n_epoch=10, validation_set=(testX, testY), show_metric=True,\n",
    "          batch_size=batch_size)\n",
    "  _y=model.predict(X)\n",
    "model.save(\"tflearn.lstm.model\")\n",
    "print (_y)\n",
    "print (y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
